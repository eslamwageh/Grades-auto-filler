{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset. This will take time ...\n",
      "[INFO] processed 1000/37949\n",
      "[INFO] processed 2000/37949\n",
      "[INFO] processed 3000/37949\n",
      "[INFO] processed 4000/37949\n",
      "[INFO] processed 5000/37949\n",
      "[INFO] processed 6000/37949\n",
      "[INFO] processed 7000/37949\n",
      "[INFO] processed 8000/37949\n",
      "[INFO] processed 9000/37949\n",
      "[INFO] processed 10000/37949\n",
      "[INFO] processed 11000/37949\n",
      "[INFO] processed 12000/37949\n",
      "[INFO] processed 13000/37949\n",
      "[INFO] processed 14000/37949\n",
      "[INFO] processed 15000/37949\n",
      "[INFO] processed 16000/37949\n",
      "[INFO] processed 17000/37949\n",
      "[INFO] processed 18000/37949\n",
      "[INFO] processed 19000/37949\n",
      "[INFO] processed 20000/37949\n",
      "[INFO] processed 21000/37949\n",
      "[INFO] processed 22000/37949\n",
      "[INFO] processed 23000/37949\n",
      "[INFO] processed 24000/37949\n",
      "[INFO] processed 25000/37949\n",
      "[INFO] processed 26000/37949\n",
      "[INFO] processed 27000/37949\n",
      "[INFO] processed 28000/37949\n",
      "[INFO] processed 29000/37949\n",
      "[INFO] processed 30000/37949\n",
      "[INFO] processed 31000/37949\n",
      "[INFO] processed 32000/37949\n",
      "[INFO] processed 33000/37949\n",
      "[INFO] processed 34000/37949\n",
      "[INFO] processed 35000/37949\n",
      "[INFO] processed 36000/37949\n",
      "[INFO] processed 37000/37949\n",
      "Finished loading dataset.\n",
      "############## Training SVM ##############\n",
      "SVM accuracy: 97.86561264822134 %\n",
      "Saved digits models to digits_models.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'SVM': LinearSVC(random_state=42)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the random seed\n",
    "path_to_digits_dataset = r'digits_dataset2'\n",
    "path_to_symbols_dataset = r'symbols_dataset'\n",
    "\n",
    "random_seed = 42  # Ensures that you (or others) can run the code multiple times and get the same results.\n",
    "target_img_size = (128, 128)\n",
    "classifiers = {\n",
    "    'SVM': svm.LinearSVC(random_state=random_seed),\n",
    "}\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    \"\"\"\n",
    "    Extracts Histogram of Oriented Gradients (HOG) features from the input image.\n",
    "    This involves resizing the image to a fixed size, dividing it into cells and blocks, \n",
    "    computing gradient histograms, and flattening the result into a feature vector. \n",
    "    The extracted features are useful for machine learning tasks like image classification and detection.\n",
    "    \"\"\"\n",
    "    img = cv2.resize(img, dsize=target_img_size)\n",
    "    win_size = (128, 128) \n",
    "    cell_size = (16, 16) #Divides the window into smaller cells (4x4 pixels per cell).\n",
    "    block_size_in_cells = (8, 8) \n",
    "\n",
    "    # divides window to blocks then blocks to cells\n",
    "    block_size = (block_size_in_cells[1] * cell_size[1], block_size_in_cells[0] * cell_size[0])\n",
    "    block_stride = (cell_size[1], cell_size[0]) #Determines how much the block moves at each step (4x4 pixels, matching cell size).\n",
    "    nbins = 9  # Number of orientation bins\n",
    "    hog = cv2.HOGDescriptor(win_size, block_size, block_stride, cell_size, nbins)\n",
    "    h = hog.compute(img)\n",
    "    h = h.flatten()\n",
    "    return h.flatten() #This array is used as input for machine learning models.\n",
    "\n",
    "def load_dataset(path_to_dataset):\n",
    "    features = []\n",
    "    labels = []\n",
    "    img_filenames = os.listdir(path_to_dataset)\n",
    "\n",
    "    for i, fn in enumerate(img_filenames):\n",
    "        if fn.split('.')[-1] != 'jpg':\n",
    "            continue\n",
    "\n",
    "        label = fn.split('.')[0]\n",
    "        labels.append(label)\n",
    "\n",
    "        path = os.path.join(path_to_dataset, fn)\n",
    "        img = cv2.imread(path)\n",
    "        features.append(extract_hog_features(img))\n",
    "        \n",
    "        # show an update every 1,000 images\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "            print(\"[INFO] processed {}/{}\".format(i, len(img_filenames)))\n",
    "        \n",
    "    return features, labels   \n",
    "\n",
    "def load_dataset_symbols(root_folder):\n",
    "    features = []\n",
    "    labels = []\n",
    "    symbol_folders = os.listdir(root_folder)\n",
    "\n",
    "    for symbol in symbol_folders:\n",
    "        symbol_folder = os.path.join(root_folder , symbol)\n",
    "        symbol_files = os.listdir(symbol_folder)\n",
    "        # print(symbol_files)\n",
    "        for filename in symbol_files:\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                img_path = os.path.join(symbol_folder, filename)\n",
    "                img = cv2.imread(img_path, 0)  # Load image in grayscale\n",
    "                if img is not None:\n",
    "                    features.append(extract_hog_features(img))\n",
    "                    # print(features)\n",
    "                    labels.append(symbol)\n",
    "    return features, labels # should it be np.array(features) ??\n",
    "\n",
    "# This function will test all our classifiers on the specifc SVM model and append the result to the SVM model \n",
    "def run_experiment(path_to_dataset, digits_or_symbols):\n",
    "    \n",
    "    # Load dataset with extracted features\n",
    "    print('Loading dataset. This will take time ...')\n",
    "    if digits_or_symbols == 0:\n",
    "        features, labels = load_dataset(path_to_dataset)\n",
    "    else:\n",
    "        features, labels = load_dataset_symbols(path_to_dataset)\n",
    "    print('Finished loading dataset.')\n",
    "    \n",
    "    # Since we don't want to know the performance of our classifier on images it has seen before\n",
    "    # we are going to withhold some images that we will test the classifier on after training \n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=random_seed)\n",
    "    digits_models = {}\n",
    "    symbols_models = {}\n",
    "    for model_name, model in classifiers.items():\n",
    "        print('############## Training', model_name, \"##############\")\n",
    "        # Train the model only on the training features\n",
    "        model.fit(train_features, train_labels)\n",
    "        \n",
    "        # Test the model on images it hasn't seen before\n",
    "        accuracy = model.score(test_features, test_labels)\n",
    "        \n",
    "        if digits_or_symbols == 0:\n",
    "            digits_models[model_name] = model\n",
    "        else:\n",
    "            symbols_models[model_name] = model\n",
    "\n",
    "        print(model_name, 'accuracy:', accuracy*100, '%')\n",
    "\n",
    "        if digits_or_symbols == 0:\n",
    "            dump(digits_models, 'digits_models.joblib')\n",
    "            print('Saved digits models to digits_models.joblib')\n",
    "        else:\n",
    "            dump(symbols_models, 'symbols_models.joblib')\n",
    "            print('Saved symbols models to symbols_models.joblib')\n",
    "    if digits_or_symbols == 0:\n",
    "        return digits_models\n",
    "    \n",
    "    return symbols_models\n",
    "\n",
    "## You do not need to call these as the model file is already provided\n",
    "run_experiment(path_to_digits_dataset, 0)\n",
    "# run_experiment(path_to_symbols_dataset, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
